## Vulnerability List for screenshot-to-code Project

* Vulnerability Name: **Exposed OpenAI API Key via Debug Logs**

* Description:
    1. An attacker can enable debug mode by setting the `IS_DEBUG_ENABLED` environment variable to `True`.
    2. When debug mode is enabled, the application writes debug logs to files in the directory specified by the `DEBUG_DIR` environment variable.
    3. If `DEBUG_DIR` is set to a publicly accessible location or if the debug logs are inadvertently exposed (e.g., through misconfigured web server or exposed directory listing), an attacker can access these log files.
    4. The debug logs, particularly those generated by `DebugFileWriter.py`, might contain sensitive information, including the OpenAI API key if it's used or echoed during debugging processes.
    5. An attacker who gains access to these logs can extract the OpenAI API key.
    6. With a valid OpenAI API key, the attacker can make unauthorized requests to the OpenAI API, potentially incurring costs for the application owner or gaining access to OpenAI's services for malicious purposes.

* Impact:
    - **High**: Unauthorized access to and potential misuse of the OpenAI API key. This can lead to financial costs for the application owner due to unauthorized API usage. It could also allow the attacker to leverage OpenAI's services for other malicious activities, indirectly attributed to the project.

* Vulnerability Rank: **High**

* Currently Implemented Mitigations:
    - The project uses environment variables (`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GEMINI_API_KEY`) to manage API keys, which is a standard security practice to avoid hardcoding secrets in the source code.
    - The `IS_DEBUG_ENABLED` flag in `config.py` controls whether debug logging is active. This allows disabling debug mode in production environments.

* Missing Mitigations:
    - **Secure handling of debug logs:** The project lacks proper safeguards for debug logs when debug mode is enabled. It should avoid logging sensitive information like API keys even in debug mode. If logging API interaction details is necessary for debugging, it should be done without revealing the actual API keys (e.g., redacting or masking the key).
    - **Restricting debug mode in production:** There should be clear guidance and potentially enforced configuration to disable debug mode in production deployments.  Ideally, the application should refuse to start if debug mode is enabled in a production environment.
    - **Secure storage for debug logs:** If debug logging is enabled in non-production environments, logs should be stored in a secure location with restricted access, not in publicly accessible directories.

* Preconditions:
    - Debug mode (`IS_DEBUG_ENABLED=True`) must be enabled, either unintentionally in production or in a non-production environment that is accessible to attackers.
    - The `DEBUG_DIR` must be set to a location that is either publicly accessible or becomes accessible due to misconfiguration.
    - Debug logging must actually log or echo the API key during operation (needs verification in code but is a plausible scenario during development/debugging).

* Source Code Analysis:

    1. **`backend/config.py`**:
        ```python
        IS_DEBUG_ENABLED = bool(os.environ.get("IS_DEBUG_ENABLED", False))
        DEBUG_DIR = os.environ.get("DEBUG_DIR", "")
        ```
        This code snippet shows that `IS_DEBUG_ENABLED` is controlled by the environment variable `IS_DEBUG_ENABLED`, and `DEBUG_DIR` by `DEBUG_DIR`.  If `IS_DEBUG_ENABLED` is true, debugging is enabled, and logs will be written to `DEBUG_DIR`.

    2. **`backend/debug/DebugFileWriter.py`**:
        ```python
        from config import DEBUG_DIR, IS_DEBUG_ENABLED


        class DebugFileWriter:
            def __init__(self):
                if not IS_DEBUG_ENABLED:
                    return

                try:
                    self.debug_artifacts_path = os.path.expanduser(
                        f"{DEBUG_DIR}/{str(uuid.uuid4())}"
                    )
                    os.makedirs(self.debug_artifacts_path, exist_ok=True)
                    print(f"Debugging artifacts will be stored in: {self.debug_artifacts_path}")
                except:
                    logging.error("Failed to create debug directory")

            def write_to_file(self, filename: str, content: str) -> None:
                try:
                    with open(os.path.join(self.debug_artifacts_path, filename), "w") as file:
                        file.write(content)
                except Exception as e:
                    logging.error(f"Failed to write to file: {e}")

            def extract_html_content(self, text: str) -> str:
                return str(text.split("<html>")[-1].rsplit("</html>", 1)[0] + "</html>")
        ```
        The `DebugFileWriter` class is responsible for writing debug information to files. The constructor checks `IS_DEBUG_ENABLED`. If true, it creates a debug directory under `DEBUG_DIR`. The `write_to_file` method writes content to a file within this debug directory.  If the `content` being written includes sensitive information like API keys, and if `DEBUG_DIR` is exposed, it leads to the vulnerability.

    3. **`backend/llm.py`**:
        While `llm.py` doesn't explicitly log API keys, it uses them to instantiate clients:
        ```python
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        client = AsyncAnthropic(api_key=api_key)
        client = genai.Client(api_key=api_key)
        ```
        If during development or debugging, logging is added around these client instantiations or within the `stream_openai_response`, `stream_claude_response`, or `stream_gemini_response` functions, and the `api_key` variable is logged directly or indirectly as part of debug information, then the API key could be exposed in the debug logs written by `DebugFileWriter`.  This is a common debugging practice that can unintentionally introduce vulnerabilities.

* Security Test Case:

    1. **Setup Debug Environment:**
        - Deploy the `screenshot-to-code` application in a test environment.
        - Set the environment variable `IS_DEBUG_ENABLED=True`.
        - Set the environment variable `DEBUG_DIR` to a publicly accessible directory within the webserver's document root (e.g., `./public_logs`). Ensure this directory is served by the webserver. If not possible to set to a public directory, assume a scenario where an attacker gains access to the server filesystem where logs are stored due to other vulnerabilities.
        - Start the backend application.

    2. **Trigger Code Generation:**
        - Access the frontend of the application via a web browser.
        - Upload a screenshot and initiate the code generation process. This should trigger backend API calls and potentially generate debug logs if logging is implemented in vulnerable locations.

    3. **Access Debug Logs:**
        - In a separate browser window or using `curl`, attempt to access the debug log directory and files. If `DEBUG_DIR` was set to `./public_logs`, try to access `http://<your-app-url>/public_logs/`. You might need to guess or enumerate filenames as they are UUID based, but predictable naming or directory listing exposure would make this easier. If assuming filesystem access, navigate to the log directory on the server.

    4. **Analyze Log Files:**
        - Open or download the log files.
        - Search the log files for the OpenAI API key or any of the other API keys (`ANTHROPIC_API_KEY`, `GEMINI_API_KEY`). Look for log entries that might inadvertently print or reveal the API key value, especially around the time of API client initialization or request logging.

    5. **Verify API Key:**
        - If an API key is found in the logs, attempt to use it to make a simple API request to OpenAI (or Anthropic/Gemini depending on which key is exposed) using `curl` or a similar tool. For example, using the OpenAI key:
          ```bash
          curl https://api.openai.com/v1/models \
            -H "Authorization: Bearer <extracted-api-key>"
          ```
        - If the API request is successful (returns a valid JSON response of models), then the vulnerability is confirmed - the API key has been successfully extracted from debug logs and can be used for unauthorized access.

* Vulnerability Name: **Path Traversal in Evals Route**

* Description:
    1. The `/evals`, `/pairwise-evals`, and `/best-of-n-evals` routes in `backend/routes/evals.py` take folder paths as query parameters (`folder`, `folder1`, `folder2`, etc.).
    2. These paths are used with `os.listdir` and `os.path.join` to access files within the specified directories.
    3. The application checks if the provided folder exists using `Path(folder).exists()`, but it does not sanitize or validate the input path to prevent path traversal attacks.
    4. An attacker can provide a crafted path like `../` in the `folder` parameter to traverse directory structure and access files and directories outside the intended evaluation directories.
    5. By exploiting this vulnerability, an attacker could potentially read sensitive files on the server.

* Impact:
    - **High**: Arbitrary file read. An attacker can read files outside of the intended evaluation directories, potentially gaining access to sensitive information like configuration files, source code, or data stored on the server.

* Vulnerability Rank: **High**

* Currently Implemented Mitigations:
    - The application checks if the provided folder exists using `Path(folder).exists()`. This is a basic check for directory existence but does not prevent path traversal.

* Missing Mitigations:
    - **Input path validation and sanitization:** The application needs to validate and sanitize the `folder`, `folder1`, `folder2`, etc. parameters to prevent path traversal. This should include:
        - Ensuring that the provided path is within the expected base directory (e.g., `EVALS_DIR`).
        - Removing or rejecting path traversal sequences like `../`.
        - Using secure path manipulation functions that prevent traversal, if available in the framework.

* Preconditions:
    - The application must be running and the `/evals`, `/pairwise-evals`, or `/best-of-n-evals` routes must be accessible.
    - An attacker needs to be able to send HTTP GET requests to these routes.

* Source Code Analysis:

    1. **`backend/routes/evals.py`**:
        ```python
        @router.get("/evals", response_model=list[Eval])
        async def get_evals(folder: str):
            if not folder:
                raise HTTPException(status_code=400, detail="Folder path is required")

            folder_path = Path(folder) # [POINT OF VULNERABILITY] Path object created from user input, not validated for traversal
            if not folder_path.exists():
                raise HTTPException(status_code=404, detail=f"Folder not found: {folder}")

            try:
                evals: list[Eval] = []
                files = {
                    f: os.path.join(folder, f) # [POINT OF VULNERABILITY] User input 'folder' used in path join without sanitization
                    for f in os.listdir(folder) # [POINT OF VULNERABILITY] User input 'folder' used directly in listdir
                    if f.endswith(".html")
                }
        ```
        The code directly uses the `folder` parameter from the query string in `os.listdir` and `os.path.join` without any validation to prevent path traversal. The `Path(folder).exists()` check only verifies if the directory exists but does not prevent accessing directories outside the intended scope using paths like `../`. This lack of input sanitization allows an attacker to manipulate the `folder` parameter to traverse the file system. This vulnerability is also present in `get_pairwise_evals` and `get_best_of_n_evals` routes which similarly handle `folder1`, `folder2`, etc. parameters.

* Security Test Case:

    1. **Deploy Application:** Deploy the `screenshot-to-code` application in a test environment.
    2. **Identify Application Root:** Determine the application's root directory on the server. Assume it's `/app`. Identify a sensitive file outside of the intended evaluation directories, for example, a known file in the parent directory of the application root (if accessible and exists), or a system file if permissions allow, like `/etc/passwd` (though reading system files might be restricted by OS). For this test case, let's assume there's a sensitive file named `sensitive_config.txt` in the parent directory of the application, located at `/sensitive_config.txt`.  If such a file is not easily available, the aim is to demonstrate directory traversal, so listing the contents of a directory outside the intended scope is sufficient to prove the vulnerability.
    3. **Craft Path Traversal URL:** Construct a URL to exploit the `/evals` endpoint with a path traversal payload. For example, if the application is running at `http://<your-app-url>`, and you want to attempt to access `/sensitive_config.txt` (assuming it is accessible from the application's context using relative paths), craft the URL like this: `http://<your-app-url>/evals?folder=../../`. This payload assumes that the intended "evals" directory is a few levels deep from the root where `sensitive_config.txt` resides. Adjust the number of `../` based on the actual directory structure. A safer approach for demonstration is to try and list a known directory, like the root directory `/`, using `folder=../../../../../../` (adjust number of `../` to reach root from application's expected eval directory). For this test, let's aim to read files from the parent directory.
    4. **Send Malicious Request:** Send a GET request to the crafted URL. For example, using `curl`:
        ```bash
        curl "http://<your-app-url>/evals?folder=../"
        ```
    5. **Analyze Response:** Examine the HTTP response. If the path traversal is successful, and if there are `.html` files in the directory being traversed (in this case, the parent directory), the response will contain a JSON list of `Eval` objects.  Even if there are no `.html` files in the parent directory, and you get an empty list or an error related to file processing within the traversed directory, it still indicates successful path traversal because the application attempted to process files in the parent directory based on user input. To confirm arbitrary file read, you would need to modify the backend to return file *content* instead of just listing filenames based on the directory, or find another endpoint that reads and returns file content based on user-controlled paths. However, the ability to list or attempt to process files in directories outside the intended scope via path traversal is itself a high-severity vulnerability.
