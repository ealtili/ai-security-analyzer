- Vulnerability Name: Cross-Site Scripting (XSS) in Generated Code

- Description:
    1. An attacker crafts a malicious screenshot or design mockup that includes text or visual elements designed to be interpreted as code by the AI model.
    2. The user uploads this malicious screenshot to the application.
    3. The backend sends the screenshot to the AI model (e.g., GPT-4 Vision, Claude) for processing.
    4. The AI model, misinterpreting the malicious elements in the screenshot as intended code, generates code that includes these elements without proper sanitization.
    5. The backend receives the generated code and passes it to the frontend. Specifically, in `backend/routes/generate_code.py`, the code from the LLM is processed by `extract_html_content` from `codegen/utils.py` which only extracts HTML without sanitization. This unsanitized HTML is then sent to the frontend via websocket in `send_message("setCode", updated_html, index)`.
    6. The frontend displays the generated code to the user, potentially allowing the malicious code (e.g., Javascript) to be executed in the user's browser when the generated code is used to build a web application.
    7. If a user uses the generated code without proper review and sanitization in their own web application, any user visiting that application could be exposed to the XSS vulnerability.

- Impact:
    - An attacker could inject malicious scripts into web applications built using the code generated by screenshot-to-code.
    - This could lead to various attacks, including:
        - Stealing user cookies and session tokens.
        - Redirecting users to malicious websites.
        - Defacing websites.
        - Performing actions on behalf of the user without their consent.
        - Harvesting user credentials.

- Vulnerability Rank: High

- Currently Implemented Mitigations:
    - None evident from the provided project files. The code generation process appears to directly translate visual elements from screenshots into code without any explicit sanitization or security checks. The `codegen/utils.py` file only extracts HTML content but does not sanitize it. The file `backend/routes/generate_code.py` uses this utility to process the generated code before sending it to the frontend, inheriting the lack of sanitization.

- Missing Mitigations:
    - **Input Sanitization:** The application should sanitize the input screenshot or design mockup to remove or neutralize any potentially malicious code elements before sending it to the AI model. However, this might be difficult to achieve effectively for image-based inputs.
    - **Output Sanitization/Encoding:** The generated code, especially HTML, CSS, and Javascript, must be rigorously sanitized or encoded to prevent XSS vulnerabilities. This could involve:
        - Encoding HTML special characters (e.g., `<`, `>`, `&`, `"`, `'`).
        - Removing or escaping Javascript event handlers (e.g., `onload`, `onclick`).
        - Using a Content Security Policy (CSP) in the generated code to restrict the capabilities of scripts.
    - **Code Review Guidance:** The application should clearly warn users about the potential security risks of using AI-generated code without thorough review and sanitization. It should recommend manual code review and security testing of the generated output before deployment.

- Preconditions:
    - The attacker needs to be able to create a malicious screenshot or design mockup.
    - A user must upload this malicious screenshot to the screenshot-to-code application and generate code.
    - A user must then use the generated code in a web application without proper security review.

- Source Code Analysis:
    - **`backend/llm.py`, `backend/evals/core.py`**: These files handle communication with the LLMs (OpenAI, Anthropic, Gemini) and retrieve the generated code as a string. There is no code in these files that performs any sanitization on the LLM's response. The code is directly passed back.
    - **`backend/codegen/utils.py`**: The `extract_html_content` function in `codegen/utils.py` is used to extract the HTML part from the LLM's response. However, this function only performs extraction using regular expressions (`re.search(r"(<html.*?>.*?</html>)", text, re.DOTALL)`) and does not sanitize the HTML content in any way. It simply returns the extracted HTML string as is or the original text if no HTML tags are found.
    - **`backend/routes/generate_code.py`**: This file is the entry point for the code generation websocket endpoint (`/generate-code`). It receives user parameters, interacts with the LLMs via functions from `llm.py` (like `stream_openai_response`, `stream_claude_response`), and then uses `extract_html_content` to process the generated code.  Crucially, after extracting the HTML content, `generate_code.py` does not perform any sanitization or encoding before sending the code to the frontend using `send_message("setCode", updated_html, index)`. This direct transmission of unsanitized HTML is the core of the XSS vulnerability.
    - **`backend/evals.py`**: This file is used for evaluating the AI models. It reads HTML files from specified folders and serves them through the `/evals` and `/pairwise-evals` endpoints. While not directly in the code generation path for end-users, it demonstrates handling of HTML content on the backend without explicit sanitization, as it reads HTML files and serves their content.
    - **`backend/main.py`, `backend/routes/generate_code.py`**: The backend receives the generated code from `llm.py` or `evals/core.py` and sends it to the frontend through API endpoints. There is no indication in `main.py` or other backend files of any middleware or functions that would sanitize the code before sending it to the frontend.
    - **`frontend/src/components/CodeEditor.tsx` (not provided but assumed based on project description):** The frontend component responsible for displaying the generated code likely renders the received code directly in the browser. If this rendering is done without proper escaping or using mechanisms to prevent script execution, it will be vulnerable to XSS if the generated code contains malicious scripts.

    **Visualization:**

    ```
    [Malicious Screenshot] --> [Backend API Endpoint (/generate-code)]
                           |
                           v
    [backend/routes/generate_code.py] --> [llm.py/evals/core.py] --> [LLM (GPT-4, Claude)]
                                                                                |
                                                                                v
                                                                        [Generated Code (potentially malicious)]
                                                                                |
    [backend/routes/generate_code.py] <-- [Generated Code] <-- [codegen/utils.py (HTML Extraction)]
                           |
                           v
    [Frontend (CodeEditor.tsx)] <-- [Generated Code]
                           |
                           v
    [User Browser - Potential XSS Execution]
    ```

- Security Test Case:
    1. **Prepare a malicious screenshot:** Create an image file (e.g., PNG, JPG) that visually represents a simple webpage. Embed a hidden XSS payload within the text content of the visual webpage in the screenshot. For example, include text that looks like normal website content but contains a hidden Javascript payload, such as:
        ```html
        <h1>Welcome to my website</h1>
        <p>This is some normal text <img src="x" onerror="alert('XSS Vulnerability!')"></p>
        ```
        Render this HTML in a browser and take a screenshot. The key is to make the XSS payload visually inconspicuous within the screenshot.
    2. **Upload the malicious screenshot:** Go to the publicly hosted version of `screenshot-to-code` (or a locally running instance). Upload the malicious screenshot using the application's interface.
    3. **Generate Code:** Select any supported stack (e.g., HTML + Tailwind) and initiate the code generation process.
    4. **Review Generated Code:** Examine the generated code outputted by the application. Check if the XSS payload from the screenshot is present in the generated HTML, CSS, or Javascript code, and if it has been encoded or sanitized in any way.
    5. **Execute Generated Code:** Copy the generated HTML code and save it as an HTML file (e.g., `test.html`). Open `test.html` in a web browser.
    6. **Verify XSS:** Observe if the Javascript payload (e.g., `alert('XSS Vulnerability!')`) executes in the browser when the HTML file is opened. If an alert box pops up, or if other malicious actions are performed (depending on the payload), it confirms the XSS vulnerability.

This test case demonstrates that if the AI model directly translates visual text content from a malicious screenshot into code without sanitization, it can lead to the generation of XSS-vulnerable code. If a user naively uses this generated code, they will introduce an XSS vulnerability into their web application.
