- Vulnerability Name: Cross-Site Scripting (XSS) in AI-Generated Code
- Description:
  - An attacker can craft a malicious screenshot that, when processed by the application, leads the AI model to generate HTML code containing embedded malicious JavaScript.
  - Step 1: The attacker designs a screenshot with UI elements and text prompts intended to induce the AI to generate HTML code containing a malicious JavaScript payload (e.g., using text like `<script>alert('XSS')</script>` within a button label or text content in the screenshot).
  - Step 2: The attacker uploads this crafted screenshot to the application through the provided interface.
  - Step 3: The backend receives the screenshot and sends it to the chosen AI model (e.g., GPT-4 Vision, Claude 3) for code generation.
  - Step 4: The AI model, interpreting the malicious elements in the screenshot, generates HTML code that includes the attacker's embedded JavaScript payload.
  - Step 5: The backend's `extract_html_content` function in `backend/codegen/utils.py` extracts the generated HTML code from the AI's response. This function does not perform any sanitization or escaping of the HTML content.
  - Step 6: The backend sends this raw, AI-generated HTML code, including the malicious script, to the frontend through the API.
  - Step 7: The frontend, upon receiving the HTML, renders it in the user's browser. If the frontend uses insecure methods like `dangerouslySetInnerHTML` in React or directly sets `innerHTML` in JavaScript without sanitization, the embedded malicious script will be executed.
  - Step 8: The malicious JavaScript code executes within the user's browser context, resulting in an XSS vulnerability. This can lead to session hijacking, account takeover, redirection to malicious sites, or other client-side attacks.
- Impact:
  - Successful XSS can have critical impact, allowing an attacker to:
    - Steal sensitive user information, including session cookies, which can lead to unauthorized account access.
    - Perform actions on behalf of the user without their consent, such as modifying data, initiating transactions, or spreading malware.
    - Redirect users to malicious websites, potentially leading to phishing attacks or malware distribution.
    - Deface the web application, damaging the application's reputation and user trust.
    - Inject further malicious scripts, including keyloggers or cryptocurrency miners, compromising the user's system.
- Vulnerability Rank: High
- Currently Implemented Mitigations:
  - None. The backend code, specifically `backend/codegen/utils.py`, lacks any HTML sanitization or encoding mechanisms. The `extract_html_content` function directly extracts and returns the HTML without modification. Review of `backend/routes/generate_code.py` confirms that the extracted HTML is sent to the frontend without sanitization.
- Missing Mitigations:
  - Backend: Implement HTML sanitization on the AI-generated code within the backend before sending it to the frontend. Use a robust HTML sanitization library like `bleach` in Python to parse and sanitize the HTML, removing or escaping potentially harmful elements (e.g., `<script>` tags, event handlers like `onload`, `onerror`, etc.) and attributes.
  - Frontend: Ensure proper and secure rendering of the HTML received from the backend. If using React, avoid `dangerouslySetInnerHTML` entirely or, if necessary, sanitize the HTML using a frontend sanitization library (like DOMPurify) before using `dangerouslySetInnerHTML`. For plain JavaScript, use secure methods to set content and avoid directly setting `innerHTML` with unsanitized data.
- Preconditions:
  - The application must be running and publicly accessible to the attacker.
  - The attacker needs to successfully craft a screenshot that tricks the AI model into generating HTML containing malicious JavaScript. This might require some experimentation to understand the AI's interpretation patterns.
  - The frontend must be vulnerable to rendering unsanitized HTML, typically by using methods that directly execute embedded scripts.
- Source Code Analysis:
  - File: `backend/codegen/utils.py`
    ```python
    def extract_html_content(text: str):
        # Use regex to find content within <html> tags and include the tags themselves
        match = re.search(r"(<html.*?>.*?</html>)", text, re.DOTALL)
        if match:
            return match.group(1)
        else:
            # Otherwise, we just send the previous HTML over
            print(
                "[HTML Extraction] No <html> tags found in the generated content: " + text
            )
            return text
    ```
    - The `extract_html_content` function utilizes a regular expression `r"(<html.*?>.*?</html>)"` to extract HTML content from the text generated by the AI model.
    - The regex captures any content enclosed within `<html>` and `</html>` tags, including the tags themselves, using `re.DOTALL` to handle multiline HTML.
    - Critically, this function performs **no sanitization** of the extracted HTML. It directly returns the matched HTML string without any encoding, filtering, or escaping of potentially malicious content.
    - If the regular expression fails to find `<html>` tags, the function simply returns the original input `text` without any HTML extraction or sanitization, which could still contain and pass through malicious scripts if the AI model outputs raw JavaScript outside of HTML tags or if previous HTML is passed through.
  - File: `backend/routes/generate_code.py`
    ```python
    # ...
    completions = [extract_html_content(completion) for completion in completions]
    # ...
    for index, updated_html in enumerate(updated_completions):
        await send_message("setCode", updated_html, index)
    # ...
    ```
    - The `stream_code` websocket endpoint in `generate_code.py` is responsible for handling code generation requests.
    - After receiving code completions from the AI model, it calls `extract_html_content` to process the generated code.
    - The output of `extract_html_content`, which is potentially malicious HTML, is directly sent to the frontend via the `setCode` websocket message without any sanitization. This is evident in the line `await send_message("setCode", updated_html, index)`.
    - This confirms that the backend does not implement any HTML sanitization before sending the AI-generated code to the frontend, making the application vulnerable to XSS if the frontend renders this unsanitized HTML.
  - Frontend Code (Not Provided):
    - While the frontend code is not provided, the vulnerability is predicated on the assumption that the frontend is rendering the HTML received from the backend in an unsafe manner. Common frontend vulnerabilities involve using methods like `dangerouslySetInnerHTML` in React or directly setting `innerHTML` in JavaScript without proper sanitization. This assumption is based on typical web application development practices where developers may overlook sanitization when dealing with seemingly "trusted" AI-generated content.
- Security Test Case:
  - Step 1: Create a malicious PNG screenshot image. This image should visually resemble a simple webpage element (e.g., a button or a text input field). Include text within the screenshot that is likely to be interpreted by the AI as HTML and JavaScript. For example, the text on a button in the screenshot could be:  `Click me <script>alert('XSS-Test')</script>`. Save this image as `malicious_screenshot.png`.
  - Step 2: Access the application in a web browser (e.g., navigate to `http://localhost:5173` if running locally).
  - Step 3: Use the application's interface to upload the `malicious_screenshot.png`.
  - Step 4: Select an output framework (e.g., "HTML + Tailwind").
  - Step 5: Initiate the code generation process by clicking the "Generate Code" or similar button.
  - Step 6: Once the code is generated and displayed in the application's UI, or if there's an option to view/download the code, carefully inspect the generated HTML source code. Look for the presence of the injected JavaScript payload, such as `<script>alert('XSS-Test')</script>`.
  - Step 7: If the malicious script is found in the generated code, attempt to execute the generated code in a browser. This could involve copying the HTML code and saving it as an HTML file, or using the application's preview functionality if available.
  - Step 8: Verify the XSS. If, upon rendering the generated code, an alert box pops up displaying "XSS-Test" (or similar), it confirms that the Cross-Site Scripting vulnerability is present. This indicates that the malicious JavaScript from the screenshot was successfully generated into the code and executed in the browser due to lack of sanitization.
