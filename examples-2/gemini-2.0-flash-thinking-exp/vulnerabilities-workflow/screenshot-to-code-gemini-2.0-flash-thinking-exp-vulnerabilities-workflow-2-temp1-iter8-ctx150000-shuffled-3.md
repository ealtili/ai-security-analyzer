### Vulnerability List

* Vulnerability Name: Prompt Injection leading to Cross-Site Scripting (XSS)
* Description:
    1. An attacker crafts a malicious screenshot or design mockup. This input is designed to include text or visual elements that, when interpreted by the AI model, are treated as instructions within the prompt.
    2. The user uploads this malicious screenshot or mockup to the application.
    3. The backend processes the input image, extracts relevant information, and constructs a prompt for the AI model. This prompt includes the content derived from the attacker's malicious input.
    4. The AI model, interpreting the injected instructions within the prompt, generates code that incorporates these unintended and potentially harmful instructions. For example, if the injected instruction is a Javascript code snippet, the generated code might include this script directly.
    5. The application streams the generated code back to the user.
    6. If a user deploys the generated code without proper review and sanitization, the injected malicious script (e.g., XSS payload) can be executed in the user's browser, potentially leading to user data theft, session hijacking, or other malicious activities.

* Impact:
    - Cross-Site Scripting (XSS): Malicious JavaScript can be injected into the generated code. When a user deploys and opens the generated code in a browser, this script can execute.
    - Data theft: An attacker could use XSS to steal user cookies, session tokens, or other sensitive data.
    - Session hijacking: By stealing session tokens, attackers can hijack user sessions and perform actions on behalf of legitimate users.
    - Defacement: Attackers could deface the web page by injecting malicious HTML and JavaScript.
    - Redirection: Malicious scripts can redirect users to attacker-controlled websites.

* Vulnerability Rank: High
* Currently Implemented Mitigations:
    - None. The application directly uses user-provided screenshot content to generate code without sanitization of input or output.
* Missing Mitigations:
    - Input Sanitization: Sanitize the input screenshot or design mockup to remove or neutralize any potentially malicious code or instructions before feeding it to the AI model. This could involve stripping HTML tags, JavaScript code, or other executable content from the text extracted from the image.
    - Output Sanitization/Encoding: Sanitize or encode the code generated by the AI model before presenting it to the user. This could involve HTML encoding, JavaScript escaping, or using Content Security Policy (CSP) headers in the generated code to restrict the execution of inline scripts.
    - User Education: Clearly warn users about the risks of prompt injection and the importance of reviewing and sanitizing the generated code before deployment.
    - Post-generation Security Analysis: Implement automated security scanning tools to analyse the generated code for potential vulnerabilities like XSS before presenting it to the user.

* Preconditions:
    - The attacker needs to be able to create a visual input (screenshot or design mockup) that includes text or visual elements that can be interpreted as instructions by the AI model.
    - The user must upload and process this malicious input using the application.
    - The user must then deploy the generated code without carefully reviewing and sanitizing it.

* Source Code Analysis:
    1. File: `backend\routes\generate_code.py`
    ```python
    @router.websocket("/generate-code")
    async def stream_code(websocket: WebSocket):
        await websocket.accept()
        ...
        params: dict[str, str] = await websocket.receive_json()
        print("Received params")
        ...
        prompt_messages, image_cache = await create_prompt(params, stack, input_mode)
        ...
        ```
        The `stream_code` function receives user input as JSON via websocket in the `params` variable. The `params` dictionary, which includes the user-provided image (as data URL), is directly passed to the `create_prompt` function.

    2. File: `backend\prompts\__init__.py`
    ```python
    async def create_prompt(
        params: dict[str, str], stack: Stack, input_mode: InputMode
    ) -> tuple[list[ChatCompletionMessageParam], dict[str, str]]:
        ...
        if params.get("resultImage"):
            prompt_messages = assemble_prompt(
                params["image"], stack, params["resultImage"]
            )
        else:
            prompt_messages = assemble_prompt(params["image"], stack)
        ...
        return prompt_messages, image_cache

    def assemble_prompt(
        image_data_url: str,
        stack: Stack,
        result_image_data_url: Union[str, None] = None,
    ) -> list[ChatCompletionMessageParam]:
        system_content = SYSTEM_PROMPTS[stack]
        user_prompt = USER_PROMPT if stack != "svg" else SVG_USER_PROMPT

        user_content: list[ChatCompletionContentPartParam] = [
            {
                "type": "image_url",
                "image_url": {"url": image_data_url, "detail": "high"},
            },
            {
                "type": "text",
                "text": user_prompt,
            },
        ]
        ...
        return [
            {
                "role": "system",
                "content": system_content,
            },
            {
                "role": "user",
                "content": user_content,
            },
        ]
    ```
        The `create_prompt` function calls `assemble_prompt`. The `assemble_prompt` function constructs the prompt messages, including the `image_data_url` which originates from `params["image"]` (user-controlled input). The content of the image is directly included in the prompt without any sanitization. The system prompt (e.g., `HTML_TAILWIND_SYSTEM_PROMPT` from `backend\prompts\screenshot_system_prompts.py`) instructs the AI on how to generate code based on the screenshot.

    **Visualization:**

    ```
    User Input (Malicious Screenshot) --> backend\routes\generate_code.py (stream_code) --> params["image"] --> backend\prompts\__init__.py (create_prompt) --> backend\prompts\__init__.py (assemble_prompt) --> Prompt to LLM (including malicious content from image) --> LLM Code Generation (potentially including malicious code) --> User Receives Code (vulnerable to XSS if deployed)
    ```

    **Explanation:** The code directly takes the user-provided image data and incorporates it into the prompt sent to the LLM. There is no sanitization or validation of the image content or the extracted text before it's used in the prompt. This allows an attacker to inject malicious instructions into the prompt via a crafted screenshot, leading to the generation of vulnerable code.

* Security Test Case:
    1. Prepare a malicious screenshot:
        - Create a simple image (e.g., using any image editor or online tool).
        - Embed the following text within the image, mimicking a UI element: `<div id="xss">Click me</div><script>document.getElementById('xss').onclick = function(){alert("XSS Vulnerability!")}</script>`
        - Save the image as `malicious_screenshot.png`.

    2. Run the screenshot-to-code application locally as described in the README.

    3. Open the frontend in a browser (usually http://localhost:5173).

    4. In the application, select "HTML + Tailwind" as the stack.

    5. Upload the `malicious_screenshot.png` using the image upload functionality.

    6. Wait for the code generation to complete and examine the generated code.  You can view the generated code in the application's UI.

    7. Look for the injected XSS payload in the generated HTML code. It might appear similar to:
        ```html
        ...
        <div id="xss">Click me</div><script>document.getElementById('xss').onclick = function(){alert("XSS Vulnerability!")}</script>
        ...
        ```
        The exact code might be slightly different depending on how the AI interprets and formats the injected script.

    8. Copy the generated HTML code.

    9. Create a new HTML file (e.g., `test_xss.html`) and paste the generated code into it.

    10. Open `test_xss.html` in a web browser.

    11. Verify if the XSS payload executes:
        - In this specific test case, you should see an alert box with the message "XSS Vulnerability!" when you click on the "Click me" text in the rendered page.

    12. If the alert box appears, it confirms that the prompt injection vulnerability exists, leading to XSS in the generated code.

This test case demonstrates how a malicious user can inject an XSS payload through a crafted screenshot, which is then incorporated into the generated code and becomes executable when the code is deployed.
