### Vulnerability List

- Vulnerability Name: Unprotected Evals Endpoints

- Description:
    The evaluation endpoints located in `backend/routes/evals.py` are publicly accessible without any form of authentication or authorization. This allows any external attacker to trigger these endpoints. Specifically, an attacker can:
    1. Access the `/models` endpoint to list available models and stacks.
    2. Trigger evaluations using the `/run_evals` endpoint, specifying models and stacks. This action results in the application making calls to configured LLM APIs (OpenAI, Anthropic, Gemini, Replicate), potentially consuming the API credits associated with the application's account.
    3. Access evaluation results via `/evals`, `/pairwise-evals`, and `/best-of-n-evals` endpoints, potentially gaining insights into evaluation data such as input images and generated code.
    This lack of protection means that malicious actors could exploit the evaluation functionality for unauthorized purposes, including resource consumption and information gathering.

- Impact:
    - **Financial Impact**: An attacker can repeatedly trigger the `/run_evals` endpoint, leading to significant consumption of API credits for LLM services (OpenAI, Anthropic, Gemini, Replicate). This could result in unexpected costs for the application owner.
    - **Information Disclosure**: Evaluation results, which may include input screenshots and the code generated by the AI models, are accessible through the `/evals`, `/pairwise-evals`, and `/best-of-n-evals` endpoints. This could expose sensitive information depending on the nature of the evaluation datasets used.
    - **Resource Exhaustion (Indirect)**: While not a direct denial of service vulnerability, continuous triggering of evaluations can indirectly exhaust backend resources and potentially degrade the application's performance for legitimate users.

- Vulnerability Rank: High

- Currently implemented mitigations:
    None. The code in `backend/routes/evals.py` defines the API endpoints using `FastAPI`'s `APIRouter` without implementing any authentication or authorization mechanisms. The `main.py` file includes `evals.router` in the application, confirming that these endpoints are exposed.
    ```python
    # backend/main.py
    app.include_router(evals.router)
    ```

- Missing mitigations:
    - **Authentication**: Implement an authentication mechanism to verify the identity of users accessing the eval endpoints. This could be done using API keys, JWT tokens, or session-based authentication.
    - **Authorization**: Implement an authorization mechanism to control access to the eval endpoints based on user roles or permissions. Restrict access to administrative users or internal services only.
    - **Rate Limiting**: Implement rate limiting on the `/run_evals` endpoint to prevent abuse by limiting the number of evaluation requests from a single IP address or user within a specific timeframe. This is a less effective mitigation compared to authentication and authorization but can help reduce the impact of abuse.

- Preconditions:
    - A publicly accessible instance of the `screenshot-to-code` application must be deployed and running.
    - API keys for at least one of the supported LLM providers (OpenAI, Anthropic, Gemini, Replicate) must be configured in the backend for the `/run_evals` endpoint to successfully trigger code generation and consume credits.

- Source code analysis:
    1. **File:** `backend/routes/evals.py`
    2. **Router Definition:** The code initializes a FastAPI `APIRouter` to handle evaluation-related API endpoints:
    ```python
    router = APIRouter()
    ```
    3. **Endpoint Definitions:** Several endpoints are defined using this router, including:
        - `/evals` (GET): Retrieves evaluation results for a given folder.
        - `/pairwise-evals` (GET): Retrieves pairwise evaluation results from two folders.
        - `/run_evals` (POST): Triggers the execution of image evaluations for specified models and stacks.
        - `/models` (GET): Returns a list of available models and stacks.
        - `/best-of-n-evals` (GET): Retrieves best-of-n evaluation results from multiple folders.
    4. **Lack of Authentication/Authorization:** None of these endpoint definitions include any decorators or middleware to enforce authentication or authorization. For example, there is no usage of `Depends` with security schemes to validate user credentials or roles before processing requests to these endpoints.
    5. **`run_evals` Endpoint Vulnerability:** The `/run_evals` endpoint directly calls `run_image_evals` function from `backend/evals/runner.py`. This function orchestrates the code generation process using configured LLM models. Without authentication, any external user can send a POST request to `/run_evals` and initiate these evaluations, leading to API credit consumption.
    ```python
    @router.post("/run_evals", response_model=List[str])
    async def run_evals(request: RunEvalsRequest) -> List[str]:
        """Run evaluations on all images in the inputs directory for multiple models"""
        all_output_files: List[str] = []

        for model in request.models:
            output_files = await run_image_evals(model=model, stack=request.stack)
            all_output_files.extend(output_files)

        return all_output_files
    ```
    6. **`evals`, `pairwise-evals`, `best-of-n-evals` Endpoint Vulnerability:** Endpoints for retrieving evaluation data do not check for any permissions, making evaluation data potentially accessible to unauthorized users if they know the folder paths.
    7. **CORS Configuration:** The `main.py` file configures CORS to allow all origins (`allow_origins=["*"]`). While CORS itself is not a security feature and doesn't prevent server-side vulnerabilities, this permissive configuration, combined with the lack of authentication on eval endpoints, widens the attack surface, making it easier for attackers from any origin to exploit the vulnerability.
    ```python
    # backend/main.py
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    ```

- Security test case:
    1. **Setup:** Ensure you have a publicly accessible instance of the `screenshot-to-code` application running. You should also have API keys configured for at least one LLM provider so that evaluations can be executed.
    2. **Access `/models` endpoint:** Open a web browser or use a tool like `curl` and send a GET request to the `/models` endpoint of your application instance. For example:
    ```bash
    curl http://<your-public-instance-ip>:7001/models
    ```
    Verify that you receive a JSON response listing available models and stacks without any authentication challenge.
    3. **Trigger `/run_evals` endpoint:** Send a POST request to the `/run_evals` endpoint. You can use `curl`, Postman, or a similar tool. The request body should be in JSON format and include a valid `RunEvalsRequest`. For example:
    ```bash
    curl -X POST -H "Content-Type: application/json" -d '{"models": ["gpt-4o-2024-11-20"], "stack": "html_tailwind"}' http://<your-public-instance-ip>:7001/run_evals
    ```
    Observe the server logs. You should see that evaluations are being triggered, and API calls to the configured LLM are being made. This indicates successful unauthorized execution of evaluations. Monitor your API usage on the LLM provider's dashboard to confirm API credit consumption.
    4. **Access `/evals` endpoint:** To test the `/evals` endpoint, you first need to have some evaluation output generated (e.g., by running step 3). After evaluations have completed and output files are generated in the `EVALS_DIR/outputs` directory, access the `/evals` endpoint with a folder path as a query parameter. For example, if the output folder is `evals_data/outputs/Jul_10_2024_gpt-4o-2024-11-20_html_tailwind`:
    ```bash
    curl "http://<your-public-instance-ip>:7001/evals?folder=evals_data/outputs/Jul_10_2024_gpt-4o-2024-11-20_html_tailwind"
    ```
    Verify that you receive a JSON response containing evaluation data (input images and outputs) without any authentication.
    5. **Repeat for other eval endpoints:** Similarly, test the `/pairwise-evals` and `/best-of-n-evals` endpoints to confirm they are also accessible without authentication.

By completing these steps, you can successfully demonstrate that the eval endpoints are unprotected and vulnerable to unauthorized access and abuse.
