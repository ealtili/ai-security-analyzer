### Vulnerability List

- Vulnerability Name: Cross-Site Scripting (XSS) in AI-Generated Code Rendering

- Description:
    1. An attacker crafts a malicious screenshot or design mockup image.
    2. The user uploads this malicious image to the application.
    3. The backend AI model processes the image and generates code, potentially including malicious JavaScript or HTML.
    4. The backend sends the AI-generated code to the frontend via WebSocket.
    5. The frontend receives this code and directly renders it in the user's browser without proper sanitization.
    6. If the AI-generated code contains malicious scripts, these scripts will be executed in the user's browser, leading to XSS.

- Impact:
    - An attacker can execute arbitrary JavaScript code in the victim's browser.
    - This can lead to session hijacking, cookie theft, redirection to malicious websites, defacement of the web page, or other malicious actions performed in the context of the user's session.
    - If the application handles sensitive user data, this data could be compromised.

- Vulnerability Rank: High

- Currently Implemented Mitigations:
    - None evident in the provided backend code. The code generation process uses LLMs to generate code from images, and the generated code is sent to the frontend. There is no code in the backend that sanitizes or checks the generated code for malicious content before sending it to the frontend via WebSocket in `backend/routes/generate_code.py`.

- Missing Mitigations:
    - **Backend Sanitization:** Implement server-side sanitization of the AI-generated code before sending it to the frontend. This could involve using a library like DOMPurify (if running Node.js backend or a Python equivalent for HTML sanitization) to parse and sanitize the HTML, removing potentially malicious JavaScript or HTML tags and attributes.
    - **Frontend Sanitization:** Even with backend sanitization, implement client-side sanitization as a defense-in-depth measure. Use a Javascript library like DOMPurify in the frontend before rendering the AI-generated code.
    - **Content Security Policy (CSP):** Implement a strict Content Security Policy to limit the sources from which scripts can be executed and to restrict inline JavaScript. This can help mitigate the impact of XSS vulnerabilities by preventing the execution of injected scripts or limiting their capabilities.
    - **Code Review and Security Testing:** Regularly review the code, especially the prompt generation and code rendering parts, and conduct security testing to identify and fix potential vulnerabilities.

- Preconditions:
    - The attacker needs to craft a malicious screenshot or mockup image that, when processed by the AI model, results in the generation of code containing XSS payloads.
    - The user must upload and process this malicious image using the application.
    - The frontend application must be vulnerable to rendering unsanitized HTML/JS received from the backend.

- Source Code Analysis:
    1. **Backend Code Generation (`backend/routes/generate_code.py`):**
        - The `stream_code` function in `backend/routes/generate_code.py` handles WebSocket connections and code generation.
        - It receives parameters from the frontend, including an image, and uses these parameters to create a prompt for the AI model using `create_prompt` function from `backend\prompts\__init__.py`.
        - It calls different LLM APIs (OpenAI, Anthropic, Gemini) based on configuration and model selection to generate code.
        - The generated code is streamed back to the frontend via WebSocket using `websocket.send_json({"type": "setCode", updated_html, index})`.
        - **Crucially, there is no sanitization or security check performed on the `updated_html` content before sending it to the frontend.**

    ```python
    @router.websocket("/generate-code")
    async def stream_code(websocket: WebSocket):
        # ...
        for index, updated_html in enumerate(updated_completions):
            await send_message("setCode", updated_html, index) # Sends code to frontend
            # ...
    ```

    2. **Frontend Code Rendering (Inferred from Description):**
        - Based on the project description, the frontend application (React/Vite) receives the AI-generated code via WebSocket messages of type `setCode`.
        - It is assumed that the frontend directly renders this received HTML code, likely by setting it as the `innerHTML` of a DOM element or using a similar method in React to display the generated code.
        - **If the frontend directly renders the code without sanitization, it becomes vulnerable to XSS.** An attacker can inject malicious JavaScript code within the screenshot, which could be then generated by the AI and executed in the user's browser when the frontend renders the code.

    **Visualization of Vulnerability Flow:**

    ```
    [Attacker] --(Malicious Screenshot)--> [User] --(Upload Image)--> [Backend]
        [Backend] --(AI Processing & Code Generation)--> [AI Model]
        [AI Model] --(AI-Generated Code with XSS Payload)--> [Backend]
        [Backend] --(WebSocket 'setCode' Message with Malicious Code)--> [Frontend]
        [Frontend] --(Unsafe Rendering)--> [User's Browser] --(XSS Execution)--> [Attacker Control]
    ```

- Security Test Case:
    1. **Precondition:** Access to a publicly available instance of the application.
    2. **Craft a Malicious Screenshot:**
        - Create a simple image (e.g., a white square).
        - Embed a basic XSS payload in the image's metadata or in text within the image that the AI model might interpret as code. For example, add text like `<img src=x onerror=alert('XSS')>` to the image. Alternatively, try simpler payloads like `<script>alert('XSS')</script>` as text in the image.
        - Save the image as PNG or JPG.
    3. **Upload the Malicious Screenshot:**
        - Open the application in a web browser.
        - Upload the crafted malicious screenshot using the application's interface.
        - Select any supported stack (e.g., HTML + Tailwind).
        - Initiate the code generation process.
    4. **Observe the Output:**
        - After the AI model processes the image and the frontend renders the generated code, observe if an alert box appears in the browser with the text 'XSS'.
        - If the alert box appears, it confirms that the XSS payload from the malicious screenshot was successfully generated into the code and executed by the browser, proving the vulnerability.
    5. **Expected Result:** An alert box with 'XSS' should appear in the user's browser, demonstrating successful XSS exploitation.
    6. **Cleanup:** Close the alert box and the browser.
