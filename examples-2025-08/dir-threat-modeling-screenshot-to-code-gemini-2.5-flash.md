APPLICATION THREAT MODEL

ASSETS
- User-provided API Keys: These include OpenAI, Anthropic, Gemini, and Replicate API keys. These are critical credentials that grant access to paid external AI services. Their compromise could lead to unauthorized billing or API abuse.
- User-provided Screenshots, Videos, and Text Prompts: This input data is used to generate code. It could contain sensitive, proprietary, or personally identifiable information (PII) belonging to the user. This now explicitly includes images and text that can be part of conversation history in update mode.
- Generated Code: The output of the AI models. This code is intended to be functional and replicate user designs. It could potentially contain vulnerabilities, malicious scripts, or inadvertently expose information from the input if logging is not handled carefully.
- Application Logic and Source Code (Backend and Frontend): This includes the intellectual property of the application itself, residing in the Python and JavaScript codebases. Its integrity is crucial for the application's proper functioning.
- Evaluation Dataset and Outputs: Located in backend/evals_data/inputs and outputs. This dataset consists of screenshots used to benchmark AI models and the corresponding generated code. It is sensitive as it represents test cases and model performance, and could contain proprietary design examples.
- Debug Logs: Generated by DebugFileWriter and fs_logging.core. These logs can contain full prompt messages (including user inputs like text and image data URLs) and AI completions, which might inadvertently store sensitive data.
- Temporary Video Files and Extracted Frames: When a video is uploaded, it's temporarily stored on the backend's file system as a raw video file, and then individual frames are extracted. These files contain sensitive user input, similar to screenshots. If `DEBUG` is enabled in `backend/video/utils.py`, extracted frames are saved to a temporary directory on the host.
- Content fetched from User-provided URLs: When a user provides a URL for a screenshot, the backend fetches content from this external URL. This content could be sensitive or malicious.

TRUST BOUNDARIES
- User's Browser (Frontend) <-> Frontend Application: The user's browser is generally untrusted and can be manipulated by the user. The frontend application running within it must validate and sanitize user interactions.
- Frontend Application <-> Backend API: This boundary separates the client-side application from the server-side logic. Data crossing this boundary (e.g., user inputs, API keys via WebSocket, generated code, user-provided URLs for screenshots) must be secured.
- Backend API <-> External AI APIs (OpenAI, Anthropic, Gemini, Replicate): The backend communicates with third-party AI services. This boundary involves trusting the external APIs for secure processing and response.
- Backend API <-> Local File System: The backend interacts with the local file system for tasks like video processing (temporary files, extracted frames), storing evaluation data, and writing debug logs.
- Backend API <-> External User-Provided URLs: The backend actively fetches content from URLs provided by the user (for screenshot mode). This introduces a new trust boundary with potentially untrusted external web servers.
- User (running locally) <-> Localhost Frontend/Backend: When run locally, the user has direct access to both the frontend and backend processes and files, making this a less strict boundary but still requiring the application to operate securely.

DATA FLOWS
- User Input (Screenshot/Video data URL, Text Prompt, API Keys via UI settings, User-provided URL for screenshot) -> Frontend.
- Frontend (Screenshot/Video data URL, Text Prompt, API Keys, Generation Type, History, User-provided URL for screenshot) -> Backend (/generate-code WebSocket, /api/screenshot POST). This flow crosses the Frontend <-> Backend API trust boundary.
- Backend (API Keys, prompt messages, image/video data, frames extracted from video) -> External AI APIs (OpenAI, Anthropic, Gemini, Replicate). This flow crosses the Backend API <-> External AI APIs trust boundary.
- External AI APIs (Generated text/code) -> Backend.
- Backend (Generated code, status messages, variant complete/error messages, chunked responses) -> Frontend (WebSocket). This flow crosses the Frontend <-> Backend API trust boundary.
- Backend (Debug logs, evaluation inputs/outputs, temporary video files, extracted video frames) -> Local File System. This flow crosses the Backend API <-> Local File System trust boundary.
- Frontend (Generated code) -> User Browser (display and rendering).
- Backend (User-provided URL) -> External Web Server (for screenshot content). This flow crosses the Backend API <-> External User-Provided URLs trust boundary.
- External Web Server (Screenshot content) -> Backend. This flow crosses the Backend API <-> External User-Provided URLs trust boundary.

APPLICATION THREATS

- Threat: API Key Disclosure via Logging or Insecure Transmission
  - Description: User-provided API keys (OpenAI, Anthropic, Gemini, Replicate) are sent from the frontend to the backend via WebSocket. The backend then uses these keys to authenticate with external AI services. Although the README states keys are stored only in the browser, the backend receives them. If the WebSocket connection is not secured (e.g., in a local development environment without HTTPS, or if a local attacker intercepts traffic) or if the backend inadvertently logs these keys as part of prompt messages or request parameters, they could be exposed. The `fs_logging.core.write_logs` function logs `prompt_messages` and `completion`. While the keys are not directly part of the `prompt_messages` assembly, the initial `params` received over WebSocket contain the keys and could be logged if not handled carefully.
  - Impact: Unauthorized access to external AI services, leading to potential financial costs for the user or service provider, and abuse of AI models.
  - Which project component is affected: `routes/generate_code.py` (ParameterExtractionStage, ParallelGenerationStage), `backend/fs_logging/core.py`, WebSocket communication.
  - Current mitigations: The application logic aims to use keys from environment variables or UI settings, implying they are not persisted on the server. The README explicitly states keys are not stored on servers for the hosted version.
  - Missing mitigations:
    - Explicitly sanitize or redact API keys from any logging mechanisms (e.g., `fs_logging.core.write_logs`, `debug.DebugFileWriter`) before writing to disk.
    - Ensure that WebSocket communication is always over WSS (Secure WebSocket) in production deployments. For local development, this is less critical but still a best practice.
  - Risk severity: High

- Threat: Sensitive User Input Disclosure via Logging
  - Description: User-provided screenshots, videos, and text prompts are transmitted to the backend and then to external AI APIs. The `fs_logging.core.write_logs` function and `debug.DebugFileWriter` write complete prompt messages and AI completions to local files. The `test_prompt_summary` file confirms that prompt summaries (which can be logged) explicitly contain "[X images]" and the full text of prompts. If users input sensitive or proprietary information (e.g., internal designs, personal data in screenshots/videos), this data could be permanently stored in logs on the host system where the backend is running. This now also includes temporary files created during video processing if `DEBUG` is enabled in `video/utils.py`.
  - Impact: Confidentiality breach of user data, exposure of proprietary designs, or PII.
  - Which project component is affected: `routes/generate_code.py` (PostProcessingStage), `backend/fs_logging/core.py`, `backend/debug/DebugFileWriter.py`, `backend/video/utils.py` (`save_images_to_tmp` if DEBUG is True).
  - Current mitigations: None explicitly mentioned for redacting sensitive content from logs.
  - Missing mitigations:
    - Implement a configurable log redaction mechanism to remove or mask sensitive portions of user input (e.g., base64 image data, video frames) before writing to logs.
    - Clearly document data retention policies for logs and offer options for users to disable or manage logging, especially for sensitive data.
    - Ensure `DEBUG` flag in `backend/video/utils.py` is set to `False` in production environments to prevent temporary storage of video frames on disk.
  - Risk severity: High

- Threat: Malicious Code Generation (Cross-Site Scripting - XSS)
  - Description: The AI-generated code (HTML, JavaScript, CSS) is directly rendered in the frontend. A malicious user could craft an input (screenshot or text prompt) designed to trick the AI into generating code that contains XSS payloads. When this code is rendered, it could execute arbitrary scripts in the user's browser context, potentially leading to session hijacking, data theft from the browser, or defacement of the application UI. The generated code also includes external CDN resources which, if manipulated by the AI, could lead to further attacks.
  - Impact: Client-side compromise, data theft, defacement, or other attacks within the user's browser.
  - Which project component is affected: Frontend (rendering of generated code), AI models (potential for generating malicious output), `routes/generate_code.py` (CodeGenerationMiddleware).
  - Current mitigations: None specifically called out for sanitizing AI generated output before rendering, beyond basic HTML extraction.
  - Missing mitigations:
    - Implement a robust HTML sanitization library on the frontend to filter out potentially malicious scripts or attributes before rendering the AI-generated code.
    - Consider rendering the generated code within a secure sandbox environment (e.g., an iframe with `sandbox` attributes) to isolate it from the main application's context.
    - Implement Content Security Policy (CSP) headers to restrict script execution origins and other resources, limiting the impact of potential XSS.
  - Risk severity: Critical

- Threat: Supply Chain Attack via External Libraries in Generated Code
  - Description: The AI models are instructed to include external libraries from CDNs (e.g., Tailwind, React, Vue, Bootstrap, Ionic, jQuery, Font Awesome, Google Fonts, ionicons). If one of these CDNs is compromised, or if a sophisticated attacker manages to trick the AI into outputting a malicious or non-standard CDN URL, the generated application could include compromised code. This could lead to client-side attacks on users who view the generated code.
  - Impact: Compromise of the user's browser, data theft, or other client-side attacks through injected malicious code from external resources.
  - Which project component is affected: AI models (prompts), Generated Code.
  - Current mitigations: The prompts specify well-known, reputable CDNs.
  - Missing mitigations:
    - Consider implementing Subresource Integrity (SRI) for critical CDN resources in the generated code where possible, to ensure that fetched resources have not been tampered with.
    - Allow users to configure local/self-hosted versions of these libraries instead of CDNs for production-like environments, reducing reliance on external infrastructure.
  - Risk severity: Medium

- Threat: Unauthorized API Usage and Cost Overrun
  - Description: A malicious actor could exploit the `/generate-code` WebSocket endpoint to send an excessive number of requests or craft computationally expensive prompts. This could lead to a denial of service for other users or significant unexpected billing costs for the user (if using their own API keys) or the service provider (if using their own hosted keys). The parallel variant generation feature increases the potential for resource consumption.
  - Impact: Financial loss due to excessive API calls, degradation of service for other users, or complete denial of service for the application.
  - Which project component is affected: `routes/generate_code.py` (WebSocket endpoint, ParallelGenerationStage), External AI APIs.
  - Current mitigations: The application itself does not have explicit rate limiting. External AI providers typically have their own rate limits.
  - Missing mitigations:
    - Implement server-side rate limiting on the `/generate-code` WebSocket endpoint to prevent abuse from a single client.
    - Implement request complexity limits or cost estimation to warn users about potentially expensive prompts or limit the number of variants/retries.
  - Risk severity: High

- Threat: Arbitrary Command Execution via Video Processing (Potential)
  - Description: The `backend/video/utils.py` script uses `moviepy.editor.VideoFileClip` which in turn relies on `ffmpeg` (or similar underlying video processing tools) typically executed via `subprocess.run`. While `moviepy` abstracts this, if a user could craft a malicious video file (e.g., specially formatted metadata or an 'ffmpeg injection' payload) that exploits a vulnerability in `moviepy` or the underlying `ffmpeg` library, it could lead to arbitrary command execution on the host system where the backend is running. Additionally, `video/utils.py` explicitly creates `tempfile.NamedTemporaryFile` and uses `mimetypes.guess_extension`. If the `mime_type` or `suffix` could be manipulated by a user-controlled `video_data_url`, it might lead to unexpected file types or locations, though `tempfile` and `uuid` largely mitigate simple path traversal.
  - Impact: Full compromise of the backend server, including data theft, system modification, or further network attacks.
  - Which project component is affected: `backend/video/utils.py` (`split_video_into_screenshots`), `moviepy` library, underlying video processing tools (e.g., `ffmpeg`).
  - Current mitigations: `tempfile.NamedTemporaryFile` is used for the video, and `uuid.uuid4()` for temporary screenshot directories, making direct path traversal difficult.
  - Missing mitigations:
    - Implement strict validation of video file formats and content to prevent malformed or malicious video files from being processed.
    - Run video processing in an isolated, sandboxed environment (e.g., a separate container or a container with highly restricted privileges) that has no access to sensitive host resources.
    - Regularly update `moviepy` and underlying video processing libraries (like `ffmpeg`) to patch known vulnerabilities.
  - Risk severity: High

- Threat: Prompt Injection (Indirect)
  - Description: Users could craft inputs (images, text prompts, or even subtle manipulations in video frames) that aim to "jailbreak" or "align-hack" the underlying AI models. This could cause the AI to deviate from its intended instructions (e.g., generating code that is not a faithful replica, includes unintended features, or even outputs harmful content) which then manifests in the generated code. While not a direct system vulnerability, it leverages the inherent nature of LLMs to manipulate output. The `test_prompts.py` file demonstrates the construction of prompts, including user text and images as part of the conversation history, which could be vectors for injection.
  - Impact: Generated code that is incorrect, contains undesirable features, or potentially harmful content, undermining the application's purpose and trustworthiness.
  - Which project component is affected: AI models (prompts), `backend/prompts/__init__.py`.
  - Current mitigations: The system prompts are designed to guide the AI towards specific, desired outputs.
  - Missing mitigations:
    - Implement output validation or filtering on the generated code to detect and potentially block or warn about outputs that deviate significantly from expected patterns or contain known harmful elements.
    - Regularly update system prompts and model configurations to address new prompt injection techniques.
  - Risk severity: Medium (inherent to LLM applications, difficult to fully mitigate)

- Threat: Server-Side Request Forgery (SSRF) via User-Provided URLs
  - Description: The `routes/screenshot.py` file includes a `normalize_url` function, indicating that the backend will fetch content from user-provided URLs (e.g., for taking a screenshot of a webpage). An attacker could provide a malicious URL pointing to internal network resources (e.g., `http://localhost:8080/admin`, `http://169.254.169.254/latest/meta-data/`) or external, untrusted services. This could allow the attacker to scan internal networks, access sensitive metadata (like AWS IAM roles), or trigger actions on other internal services (e.g., port scanning, internal API calls, or DoS attacks). The `normalize_url` function explicitly preserves `http://` and `https://` but blocks `ftp://` and `file://`.
  - Impact: Access to internal network resources, exposure of sensitive server metadata (e.g., cloud instance credentials), or initiation of attacks against other internal/external systems.
  - Which project component is affected: `routes/screenshot.py` (`normalize_url`, and the unshown code that uses it to fetch content).
  - Current mitigations: `normalize_url` blocks `ftp://` and `file://` protocols, which is a basic step.
  - Missing mitigations:
    - Implement a robust allow-list or block-list approach for URLs that can be fetched, specifically restricting access to private IP ranges (e.g., 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 127.0.0.1/8, link-local addresses).
    - If fetching from external URLs is required, use a dedicated, isolated service or proxy for this purpose, with strict network egress controls.
    - Implement timeouts for URL fetching to prevent DoS attacks through slow responses.
  - Risk severity: Critical

- Threat: Denial of Service (DoS) via Malicious Video Upload
  - Description: The `backend/video/utils.py` processes uploaded video files. A malicious actor could upload extremely large video files, very long videos, or videos specifically crafted to be computationally expensive to process (e.g., high frame rate, complex codecs). This could exhaust disk space (for temporary files), CPU, or memory resources on the backend server, leading to a denial of service for legitimate users. The `TARGET_NUM_SCREENSHOTS` limits the number of frames extracted, but the overall video size and processing time are still unconstrained.
  - Impact: Resource exhaustion (disk, CPU, memory), leading to application unresponsiveness or complete denial of service.
  - Which project component is affected: `backend/video/utils.py` (`split_video_into_screenshots`), Backend server resources.
  - Current mitigations: `TARGET_NUM_SCREENSHOTS` caps the number of frames extracted, but not the processing of the entire video file itself. `tempfile.NamedTemporaryFile` is used for the video file, which helps with cleanup but doesn't prevent resource exhaustion during processing.
  - Missing mitigations:
    - Implement file size limits for video uploads on the frontend and backend.
    - Implement time limits for video processing operations.
    - Use resource limits for the process handling video processing (e.g., in a container or separate worker process).
  - Risk severity: High

DEPLOYMENT THREAT MODEL

Deployment Architecture: Docker/Docker Compose
I will focus on the Docker/Docker Compose deployment as it represents a more structured, albeit local, deployment model.

ASSETS
- Docker Images (backend, frontend): The built images containing the application code and dependencies. Their integrity is crucial.
- .env File: This file contains sensitive API keys (OpenAI, Anthropic, Gemini, Replicate) and other configuration variables. Its confidentiality is paramount.
- Running Containers (backend, frontend): The isolated runtime environments for the application components. Their integrity and isolation from the host system are critical.
- Host Machine Resources: CPU, memory, network bandwidth, and disk space of the machine running Docker. Availability of these resources is important for the application's performance and stability.
- Persistent Data (if configured): If volumes are mounted for debug logs or evaluation data, these files (potentially containing sensitive user inputs and generated code) become persistent assets on the host machine.
- Temporary Files on Host File System: Video processing creates temporary video files and extracted image frames on the host's temporary directory (`tempfile.gettempdir()`). These files are transient but contain sensitive user input.

TRUST BOUNDARIES
- Host OS <-> Docker Daemon: The Docker daemon runs with elevated privileges on the host and manages containers.
- Docker Daemon <-> Backend Container: The backend container is isolated from the host by Docker, but the Docker daemon itself is a privileged component.
- Docker Daemon <-> Frontend Container: Similar to the backend container, the frontend container is managed by the Docker daemon.
- Backend Container <-> External AI APIs: Communication from the backend container to external AI services.
- Frontend Container <-> User's Browser: The frontend application served from its container to the user's browser.
- User running `docker-compose` <-> .env file: The user provides the .env file to docker-compose.
- Backend Container <-> Host Temporary File System: The backend container interacts with the host's temporary file system to store video files and extracted frames during processing.

DEPLOYMENT THREATS

- Threat: API Key Exposure in .env File
  - Description: The `.env` file, which contains sensitive API keys, is read by `docker-compose`. If this file has overly permissive file system permissions on the host machine, other users or processes on the same system could read its contents.
  - Impact: Compromise of API keys, leading to unauthorized use of external AI services and potential financial costs.
  - Which project component is affected: `.env` file, Host File System.
  - Current mitigations: The `.env` file is external to the Docker images and is explicitly loaded at runtime, which prevents baking keys into the image.
  - Missing mitigations:
    - Recommend strict file permissions for the `.env` file (e.g., readable only by the owner) in deployment instructions.
  - Risk severity: High

- Threat: Container Escape via Vulnerable Dependencies or Application Code
  - Description: A vulnerability within the application code or one of its many dependencies (Python packages, Node.js packages, or even base OS libraries within the slim Docker images), particularly `moviepy` and its underlying `ffmpeg` library used for video processing, could be exploited to break out of the Docker container and gain unauthorized access to the host system.
  - Impact: Full compromise of the host machine, leading to data theft, system modification, or further attacks on the network.
  - Which project component is affected: Docker containers (backend, frontend), `Dockerfile`s, `pyproject.toml`, `package.json`, `backend/video/utils.py` (and its dependencies).
  - Current mitigations: Using slim base images (`python:3.12.3-slim-bullseye`, `node:22-bullseye-slim`) which have a smaller attack surface.
  - Missing mitigations:
    - Implement regular vulnerability scanning of Docker images and their dependencies, paying special attention to multimedia processing libraries like `moviepy` and `ffmpeg`.
    - Run containers with the least privilege necessary (e.g., non-root user).
    - Utilize Docker security features like seccomp profiles, AppArmor, or SELinux policies if deploying to a multi-tenant or sensitive environment.
    - Consider isolating video processing into a separate, more heavily sandboxed container or environment.
  - Risk severity: High

- Threat: Resource Exhaustion of Host System
  - Description: A malicious user or a bug in the application (e.g., an infinite loop in AI processing, excessive image/video processing, or logging) could cause one or both Docker containers to consume an unconstrained amount of CPU, memory, or disk space on the host machine. Video processing, in particular, is resource-intensive and could be exploited by uploading large or complex video files. This could lead to a denial of service for the application or other services running on the same host.
  - Impact: Application unresponsiveness, host system instability, or denial of service.
  - Which project component is affected: Docker containers (backend, frontend), Host system, `backend/video/utils.py`.
  - Current mitigations: None explicitly defined in `docker-compose.yml` for resource limits. `TARGET_NUM_SCREENSHOTS` caps the number of frames extracted, but the overall video processing load is still a concern.
  - Missing mitigations:
    - Configure resource limits (CPU, memory, disk I/O) for containers in `docker-compose.yml` to prevent a single container from monopolizing host resources.
    - Implement monitoring and alerting for container resource usage.
    - Implement limits on video file sizes and processing times.
  - Risk severity: High

- Threat: Exposed Ports to Untrusted Networks
  - Description: The `docker-compose.yml` explicitly maps container ports (7001 for backend, 5173 for frontend) to the host machine's ports. If the host machine is publicly accessible without proper network firewalls, these services could be exposed to the internet, allowing unauthorized access to the application endpoints.
  - Impact: Unauthorized access to the application, potential exploitation of application vulnerabilities, or use of the service for malicious purposes.
  - Which project component is affected: `docker-compose.yml` (`ports` section).
  - Current mitigations: This is primarily a local development setup, where exposure to the internet is not typically the default.
  - Missing mitigations:
    - In any deployment intended for non-local use, ensure host firewalls are configured to restrict access to these ports to trusted networks or specific IP addresses.
    - For public deployments, use a reverse proxy (e.g., Nginx, Caddy) to manage external access, handle TLS, and provide additional security layers.
  - Risk severity: Medium (High if deployed publicly without proper network controls)

- Threat: Insecure Persistent Data (Logs, Evals, Temporary Files)
  - Description: The application generates debug logs and evaluation outputs to the local file system (`DEBUG_DIR`, `EVALS_DIR`). Additionally, video processing creates temporary video files and extracted frames in the host's temporary directory (`tempfile.gettempdir()`). If these directories or temporary files have overly permissive host file system permissions, sensitive information (user inputs, generated code, video frames) could be exposed to other users or processes on the host. If volumes are not used, this data is ephemeral within the container, which is good for confidentiality but means data loss upon container termination. The `video/utils.py` explicitly deletes the `temp_video_file` but creates a temporary directory for screenshots if `DEBUG` is true, which is not explicitly deleted.
  - Impact: Confidentiality breach of user data, proprietary designs, or PII stored in logs, evaluation results, and temporary video processing files.
  - Which project component is affected: `backend/config.py` (DEBUG_DIR), `backend/evals/config.py` (EVALS_DIR), `fs_logging.core.py`, `DebugFileWriter.py`, `backend/video/utils.py` (temporary files).
  - Current mitigations: `docker-compose.yml` does not specify volumes for these directories, so data is ephemeral by default.
  - Missing mitigations:
    - If persistence is desired, clearly document the need for secure volume mounting and host file permissions.
    - Ensure temporary files created during video processing (especially the screenshot directory when `DEBUG` is true) are promptly and securely deleted after use.
    - Ensure `DEBUG` flag in `backend/video/utils.py` is `False` in production to avoid writing extracted frames to disk.
  - Risk severity: Medium

BUILD THREAT MODEL

ASSETS
- Source Code (frontend, backend): The entire codebase, including `pyproject.toml`, `package.json`, Dockerfiles, etc. Its integrity is paramount.
- Build Artifacts: Compiled frontend assets, Python virtual environment/installed packages, and Docker images. The authenticity and integrity of these artifacts are critical for secure deployment.
- Dependency Packages: Third-party libraries and tools pulled from registries like PyPI and npm. This now includes `moviepy` and its implicit dependency on `ffmpeg`. Their integrity and freedom from vulnerabilities are essential.
- Build Environment: The machine or CI/CD runner where the build process takes place. Its isolation and security are vital to prevent malicious injection into artifacts.
- API Keys/Secrets (if used during build): Any credentials required during the build process (e.g., for testing against live APIs, or for publishing to registries). Their confidentiality is critical.

TRUST BOUNDARIES
- Developer Machine <-> Git Repository: Code is pushed and pulled from the repository.
- Developer Machine <-> Package Registries (PyPI, npm): Developers install dependencies.
- Build Environment <-> Package Registries: Build process fetches dependencies.
- Build Environment <-> Docker Hub/Registry (if publishing): Built Docker images are pushed to a registry.

BUILD THREATS

- Threat: Compromised Dependencies (Supply Chain Attack)
  - Description: Malicious code could be introduced into the project via compromised third-party packages from PyPI (`poetry install`) or npm (`yarn install`). An attacker could publish a malicious package with the same name or a similar name to a legitimate dependency, or compromise a legitimate package's maintainer account. The addition of `moviepy` and its implicit dependency on `ffmpeg` (often installed as an OS package) introduces a new set of dependencies that need to be secured and monitored.
  - Impact: Introduction of backdoors, data exfiltration, or other malicious functionality into the application, affecting both developers and end-users.
  - Which project component is affected: `poetry.lock`, `pyproject.toml`, `yarn.lock`, `package.json`, `backend/Dockerfile`, `frontend/Dockerfile`, `moviepy` library, `ffmpeg` (external tool).
  - Current mitigations: Using `poetry.lock` and `yarn.lock` helps ensure reproducible builds with specific dependency versions, mitigating some risks but not against initial compromise.
  - Missing mitigations:
    - Implement automated dependency vulnerability scanning (e.g., Snyk, Dependabot, Trivy) in the build process, including scanning for OS-level dependencies like `ffmpeg` if it's installed as part of the Docker image build.
    - Use private package registries or enforce strict package provenance where possible.
    - Regularly review and audit direct and transitive dependencies.
  - Risk severity: High

- Threat: Insecure Build Environment
  - Description: If the build process (e.g., `docker-compose build` or local `poetry install`/`yarn install`) occurs on a compromised developer machine or an insecure CI/CD runner, an attacker could inject malicious code into the build artifacts (Docker images, frontend bundles). This compromised artifact would then be deployed.
  - Impact: Deployment of malicious code into the application, potentially leading to system compromise, data theft, or service disruption.
  - Which project component is affected: Build environment (developer machine, CI/CD runner).
  - Current mitigations: None explicitly mentioned in the provided files.
  - Missing mitigations:
    - Ensure build environments are clean, isolated, and regularly scanned for malware.
    - Implement strong access controls for CI/CD systems.
    - Use ephemeral build environments that are destroyed after each build.
  - Risk severity: High

- Threat: Lack of Security Scanning during Build
  - Description: The provided files do not explicitly mention the use of static application security testing (SAST) tools, dynamic application security testing (DAST) tools, or dependency vulnerability scanners during the build process. This means that security flaws in the application code (e.g., potential SSRF in URL fetching, or command injection in video processing) or known vulnerabilities in third-party dependencies might go undetected until runtime or even production.
  - Impact: Deployment of vulnerable code, increasing the risk of exploitation and compromise.
  - Which project component is affected: Source code (backend, frontend), Dependencies.
  - Current mitigations: `poetry run pyright` for type checking, `poetry run pytest` for backend tests, `yarn test` for frontend tests. These are functional/quality tests, not security-focused.
  - Missing mitigations:
    - Integrate SAST tools into the build pipeline to analyze custom code for common vulnerabilities (e.g., SQL injection, XSS, SSRF, insecure deserialization).
    - Integrate dependency vulnerability scanners to identify known vulnerabilities in third-party libraries including `moviepy` and underlying multimedia libraries.
    - Consider linting and code quality checks with security-focused rules.
  - Risk severity: High

- Threat: Unverified Base Images for Docker
  - Description: The Dockerfiles use public base images (`python:3.12.3-slim-bullseye`, `node:22-bullseye-slim`). While `slim-bullseye` indicates a minimal image, relying on public images without verification or scanning introduces a dependency on the image provider's security practices. A compromised base image could introduce vulnerabilities into the application containers.
  - Impact: Introduction of vulnerabilities into the deployed application, potentially leading to container compromise.
  - Which project component is affected: `backend/Dockerfile`, `frontend/Dockerfile`.
  - Current mitigations: Using `slim` versions which generally have fewer packages and a smaller attack surface.
  - Missing mitigations:
    - Use image scanning tools (e.g., Trivy, Clair) to scan base images and final application images for known vulnerabilities.
    - Consider using official, signed base images from trusted registries where available.
    - Regularly update base images to their latest secure versions.
  - Risk severity: Medium

- Threat: Build Artifact Tampering
  - Description: If the Docker images or other build artifacts are not cryptographically signed or verified upon completion, an attacker could potentially tamper with them after the build but before deployment. This could involve injecting malicious code into the images.
  - Impact: Deployment of compromised application versions, leading to system compromise or data breaches.
  - Which project component is affected: Built Docker images, compiled frontend assets.
  - Current mitigations: None explicitly mentioned in the provided files.
  - Missing mitigations:
    - Implement image signing (e.g., Notary, Cosign) for Docker images to ensure their integrity and authenticity from build to deployment.
    - Verify signatures before deploying artifacts.
  - Risk severity: Low (less likely for local dev, but critical for production CI/CD)

QUESTIONS & ASSUMPTIONS

QUESTIONS
- Is the hosted version (screenshottocode.com) using a similar architecture, or is it significantly different in terms of infrastructure, security controls, and data handling?
- Are there any plans for CI/CD pipelines, and if so, what environment will they run in and what security gates will be included?
- Are there specific data retention policies for user inputs (screenshots, videos, prompts) and generated code, especially for debug logs and evaluation results, and temporary files created during video processing?
- Are there any plans for robust input validation/sanitization on the backend for prompts and URLs before sending to AI or processing, specifically for URLs provided for screenshots to mitigate SSRF risks?
- Is the frontend rendered in a standard browser sandbox, or is there a desktop application wrapper (e.g., Electron) that might alter local file access permissions?
- Are resource limits applied to Docker containers in production deployments to prevent resource exhaustion, especially for video processing workloads?
- Are there any plans for output code sanitization or rendering within a strict sandbox (e.g., iframe with `sandbox` attribute) on the frontend to mitigate XSS risks from AI-generated code?
- What are the specific requirements for external URL fetching for screenshot functionality? Is it intended to fetch from any arbitrary URL, or only from trusted domains?
- How are temporary files generated during video processing handled in production environments (e.g., deletion, permissions, storage location)?

ASSUMPTIONS
- The local Docker/development setup is primarily for individual developer use and is not intended for production public-facing deployment without additional hardening and security configurations.
- The "hosted version" (screenshottocode.com) is assumed to have more robust security controls and deployment practices than the open-source local setup, although details are not available in the provided files.
- External AI APIs (OpenAI, Anthropic, Gemini, Replicate) are trusted to handle data securely and respond as expected, operating within their own security models and compliance frameworks.
- The frontend application runs within a standard browser security sandbox, limiting its direct interaction with the user's local file system.
- HTTPS is used for the hosted version, but for local `localhost` deployments, communication is typically over HTTP (for the frontend-backend connection), making it susceptible to local network sniffing if not protected.
- The backend, when fetching URLs for screenshots, is expected to fetch content from external web servers, potentially untrusted ones.
- The `DEBUG` flag in `backend/video/utils.py` is assumed to be `False` in production environments to prevent the persistence of extracted video frames on disk, but this is an assumption that needs verification.
