# APPLICATION THREAT MODEL

## ASSETS
1. Application Source Code (Python files, Dockerfile, etc.): The custom logic for the Fabric Agent Action. If compromised, an attacker could alter functionality or introduce malicious code.
2. Secrets (OPENAI_API_KEY, OPENROUTER_API_KEY, ANTHROPIC_API_KEY): These environment variables are critical for accessing LLM services. Unauthorized access to them would allow an attacker to invoke LLMs at the repository owner’s cost or exfiltrate sensitive data.
3. User Input Data: The content from GitHub issue comments, pull requests, and push events. Malicious input could lead to undesired LLM calls, unexpected costs, or code injection attempts.
4. Output Data (LLM Responses): The text generated by LLMs. If manipulated, it could misinform users or introduce malicious instructions into the GitHub repository (e.g., malicious suggestions for code changes).

## TRUST BOUNDARIES
1. GitHub Event Inputs → Action Code: Untrusted boundary where user comments/PR metadata come from potentially untrusted contributors (especially in public repos).
2. Action Code → LLM Provider: Boundary between the local action environment and the external LLM API services (OpenAI, Anthropic, OpenRouter).
3. GitHub Secrets Store → Action Code: Boundary between GitHub’s secure secrets storage and the container environment for the Action.

## DATA FLOWS
1. User/Contributor → GitHub (issue, PR, push, or comment) → Action Code (crossing trust boundary).
2. Action Code → LLM Provider (external API, crossing trust boundary).
3. LLM Provider → Action Code (response from external API, crossing trust boundary).
4. Action Code → GitHub Output (via comment or PR update).

Data flows #1, #2, and #3 all cross trust boundaries. Data flow #4 typically returns data to the same environment (GitHub), but it is also relevant since the results are posted in a public or semi-public channel.

## APPLICATION THREATS

| THREAT ID | COMPONENT NAME | THREAT NAME | STRIDE CATEGORY | WHY APPLICABLE | HOW MITIGATED | MITIGATION | LIKELIHOOD EXPLANATION | IMPACT EXPLANATION | RISK SEVERITY |
|---|---|---|---|---|---|---|---|---|---|
| 0001 | Action Code (entrypoint, Python logic) | Unauthorized user triggers Action with malicious input | Spoofing | In public repositories, anyone can comment on issues, potentially triggering an LLM-run job | Partial mitigation: workflow conditions (checking comment author, if statements in workflows) | Expand strict checks on comment author, limit allowed users to trigger the Action, or scope event triggers (as recommended in the README's "Security" section) | Likelihood is medium (public repos see frequent drive-by spam) | Could lead to unwanted LLM usage, API cost consumption, or malicious content posted by the bot | Medium |
| 0002 | Docker container (entrypoint.sh) | Altering CMD to intercept secrets | Tampering | If attacker modifies Docker image or entrypoint script, they can log or exfiltrate secrets | Not explicitly mitigated by the code but Docker image is built in a controlled environment by GitHub Actions with official checks | Implement signing and verification of Docker images, scanning in CI for changes, enforce pull from trusted source only | Low (modifying official image requires write access to the repo or pipeline) | If successful, attacker could exfiltrate secrets or run arbitrary commands | High |
| 0003 | Action Code (LLM request logic) | Manipulating requests or responses to/from LLM to inject commands | Tampering | Data in transit or at rest could be modified if an attacker sits in the middle or modifies code | TLS used by default for calls to LLM, no direct reference to insecure channels in code | Confirm all external LLM calls are HTTPS, sign requests if possible, or at least rely on LLM vendors’ TLS | Low (TLS is standard, ephemeral environment) | If an attacker can tamper with LLM requests or responses, they could produce harmful or misleading output | Medium |
| 0004 | Action Code (logging, debug) | Leaking environment variables or secrets in logs | Information Disclosure | If debug mode prints environment variables or token context, logs may leak secrets publicly | Some references in README about debug modes, but no direct evidence of secrets being redacted in code | Add environment variable masking in workflows, ensure logs do not dump secret values, verify code does not print environment variables | Medium (developer might enable debug logs inadvertently) | Could allow an adversary to see API keys from logs in the repository or console output | High |
| 0005 | GitHub Secrets Store → Action Code | Unauthorized secrets exfiltration by malicious PR from fork | Information Disclosure | Fork-based PRs can run workflows if not restricted, possibly exfiltrating secrets by printing them | The README addresses restricting secrets on PR from forks, with conditions in the workflow | Further limit actions to run only on branches in the same repo, or check user is the repo owner | High (common tactic in open-source to attempt exfiltration) | Loss of OpenAI or Anthropic keys, leading to cost and reputational damage | High |
| 0006 | Action Code → LLM Provider | Overload / excessive calls leading to cost spikes for user | Denial of Service | Attackers could cause repeated triggers, inflating usage and costs | Partly mitigated by checking event triggers (some if conditions). Not strongly enforced in code itself | Rate-limiting: implement usage checks, add cost-limits or usage-limits in the code or via provider settings | Medium (anyone can keep commenting to cause triggers if not well guarded) | Could cause unexpected large bills for LLM usage | Medium |
| 0007 | Workflow Execution (Python processes) | Execution of malicious code if an attacker modifies input data stored as code or malicious patterns | Elevation of Privilege | If the code or patterns from outside are run dynamically, attacker might break out of Python sandbox | Not explicitly mitigated, but project doesn’t appear to execute user-supplied code except via LLM patterns (which mostly returns text) | Use code scanning to ensure there is no dynamic eval of user prompts, carefully sanitize user input, reduce the risk of shell injection | Low (the code doesn’t do local “eval” of user content, but some risk remains if LLM instructions can trigger system calls) | Potentially large if shell commands were run, but code does not appear to do direct OS calls from user text | Medium |
| 0008 | Issue comment text → LLM → GitHub comment | Potential malicious instructions auto-posted back | Repudiation | A user might claim the bot posted malicious or defamatory content, blaming the repository owner | The logs are in GitHub Actions, referencing the actor who triggered the run, and the code version is stored in version control | Keep audit logs, ensure GitHub’s standard logs are not manually overwritten, use ephemeral runner logs | Low (repudiation would require forging logs) | If unaddressed, reputational damage to maintainers could be severe | Medium |

---

# DEPLOYMENT THREAT MODEL

(Note: The project can be deployed primarily as a GitHub Action, or it can be run locally via Docker. We focus on the GitHub Action deployment model.)

## ASSETS
1. GitHub Action Container Image (published to ghcr.io): If tampered with, it can compromise all repositories using it.
2. Deployment Workflow Files (.github/workflows/ci.yaml, publish.yaml, etc.): Manages build and deploy steps. If altered, an attacker can push malicious images.
3. GitHub Infrastructure (Runners, environment): Infrastructure that runs the workflow. If compromised, attacker can access logs, secrets, or code.

## TRUST BOUNDARIES
1. GitHub-hosted Runner → GitHub Container Registry: The boundary where the built container image is pushed to GHCR.
2. Public GitHub Actions → Repositories that use the published Docker image: Repos that rely on the published “fabric-agent-action” container trust that the image is unmodified and secure.

## DEPLOYMENT THREATS

| THREAT ID | COMPONENT NAME | THREAT NAME | WHY APPLICABLE | HOW MITIGATED | MITIGATION | LIKELIHOOD EXPLANATION | IMPACT EXPLANATION | RISK SEVERITY |
|---|---|---|---|---|---|---|---|---|
| 0001 | Docker Image (ghcr.io/xvnpw/fabric-agent-action) | Malicious Image Injection | Attackers might replace the Docker image stored in GHCR with malicious code, or create a similarly-named image | Publish workflow uses GitHub-provided authentication, so an attacker cannot push without write access | Enforce branch protection, sign Docker images, and use separate secrets for publishing | Low (requires compromised credentials or GitHub account) | High impact, as many downstream repos could pull the malicious image | High |
| 0002 | Publish Workflow (publish.yaml) | Tampering with Workflow to skip security checks | Attackers with write access might remove steps that check security, letting them push malicious images | Branch protection rules partly mitigate. Code owners can review changes to workflow files. | Keep code owners approvals for workflow changes enabled, set required approvals before merging | Low (requires privileged access) | Potentially large, as it can sabotage the entire release process | High |
| 0003 | GitHub-hosted Runners | Unauthorized data access to build artifacts or secrets | If a malicious actor gains control of the runner, they can see environment secrets or modify the built container | Short-lived ephemeral runners by GitHub. Zero trust approach. Each job runner is newly provisioned. | Possibly self-hosted runners must be locked down (if used). Monitor logs and revoke runner tokens quickly | Low (GitHub ephemeral runners are well hardened) | High if successful, results in secret disclosure or malicious container release | Medium |
| 0004 | Docker Image Tagging | Using old or unmaintained tag that has vulnerabilities | Users might rely on an older version with known vulnerabilities | Some mention of automated builds and test steps, but not specifically addressing old tags in code. | Institute regular scans, CVE monitoring, and encourage users to keep updating to the newest tag | Medium (some users pin old tags) | Medium if vulnerabilities remain unpatched in older images | Medium |

---

# BUILD THREAT MODEL

(This project is built via GitHub Actions. It uses two main workflows: ci.yaml (for testing + building) and publish.yaml (for pushing Docker images).)

## ASSETS
1. Build Scripts (Dockerfile, entrypoint.sh, etc.): If compromised, malicious code or backdoors may be introduced into final images.
2. CI Configuration (ci.yaml, publish.yaml): Contains logic for building, testing, and publishing. If modified incorrectly, can skip security checks or leak secrets.
3. Supply Chain Dependencies (Python libraries, Poetry-based environment): If any dependencies are compromised, malicious code can enter the final build.

## TRUST BOUNDARIES
1. GitHub Actions Build Environment → Docker Hub or GHCR (image registry)
2. Dependencies from PyPI → Build Container
3. GitHub Marketplace Actions → Build Workflow (actions/checkout, hadolint/hadolint-action, etc.)

## BUILD THREATS

| THREAT ID | COMPONENT NAME | THREAT NAME | WHY APPLICABLE | HOW MITIGATED | MITIGATION | LIKELIHOOD EXPLANATION | IMPACT EXPLANATION | RISK SEVERITY |
|---|---|---|---|---|---|---|---|---|
| 0001 | Poetry / Pypi Dependencies | Injection of malicious code through compromised dependency | The project depends on multiple PyPI libraries. If an attacker injects malicious code in a dependency, it can end up in the final image | The CI does ruff, bandit scanning, but that does not guarantee safe dependencies | Use a dependency lock (poetry.lock) and check if the dependency source is official and pinned. Add supply chain scanning / SAST for third-party libs | Medium (supply chain attacks happen frequently) | Potentially high if the malicious dependency is used widely and can exfil secrets or modify code | High |
| 0002 | Build Scripts (Dockerfile) | Unauthorized changes to Dockerfile introducing vulnerabilities | If Dockerfile is changed to run harmful commands, the final container can be compromised | Linter checks (hadolint) and code reviews. GitHub requires pull-request checks for main branch merges. | Maintain code owners on Dockerfile for mandatory manual review. Keep hadolint checks mandatory. | Low (repo write access is needed) | If not caught, malicious base image or code can be delivered. | High |
| 0003 | CI Workflow (ci.yaml) | Bypassing security checks or tests in CI pipeline | If an attacker modifies ci.yaml to skip tests or security scans, they can push unverified code | Branch protections and mandatory checks mitigate this. Code owners for workflow changes is recommended. | Enforce that changes to workflows also require admin and code owner review. Keep logs of all pipeline runs. | Low (requires privileged write access) | High impact if malicious code is merged. | High |
| 0004 | Publish Workflow (publish.yaml) | Unauthorized image publishing or version tagging | If not locked down, an attacker can push images with malicious tags or skip version increments to hide changes | The publish workflow is triggered only on push with new tags, plus standard GitHub controls. | Use limited permission tokens for publishing, verify the exact tags to be published, use ephemeral environment variables. | Medium (someone with collaborator access can attempt a malicious push) | High impact: a malicious image in GHCR can compromise many downstream users. | High |
| 0005 | Automated SAST / linting | Over-reliance on partial scans missing new vulnerabilities | SAST tools (Bandit, MyPy, RUFF) do not provide full coverage, a hidden backdoor could slip through. | They do partial checks but skip deeper dynamic checks. Code review is the manual layer. | Add more thorough scanning or auditing, possibly a container scanning stage to ID vulnerabilities. | Medium (SAST is partial but the project is not huge) | Could lead to unnoticed vulnerabilities or secrets leakage. | Medium |

---

# QUESTIONS & ASSUMPTIONS
1. We assume that GitHub Actions are restricted on fork PRs, as documented in the repository’s README. If not, threat likelihood for secrets exfiltration is even higher.
2. We assume that environment variables containing API keys are never printed in logs, and debug logs also exclude them. If that assumption fails, the risk of secret leakage is higher.
3. We assume that LLM providers (OpenAI, Anthropic, etc.) are trusted to handle requests securely over TLS.
4. We assume that the Docker image is built only via official GitHub workflows with code owners reviewing changes.
5. We assume that ephemeral GitHub-hosted runners are used instead of self-hosted runners. Self-hosted runners would introduce additional threats around local host security.
6. We assume the repository is not storing private user data requiring regulatory compliance (PII, HIPAA data, etc.), which would add significant compliance requirements.

These assumptions help scope the threat model to typical open-source usage where the main concerns revolve around cost exposure, secret keys, malicious PR interactions, supply chain attacks, and Docker image releases.
